{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyYYC: Intro to Scikit-Learn\n",
    "by matt whiteside, adapted from: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a machine learning model is a multi-step process:\n",
    "1. Feature extraction\n",
    "2. Feature selection\n",
    "3. Training\n",
    "4. Evaluation\n",
    "\n",
    "but wait! there are many different combinations of algorithms and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First example - training a classifier\n",
    "\n",
    "# Import our libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Lets test out one of datasets in sklearn\n",
    "# The Newsgroup dataset\n",
    "\n",
    "# Newsgroups are a collection of online messages organized by a subject or category\n",
    "# Can we predict the category from the content of message?\n",
    "\n",
    "# Let's pick out a few categories to work with\n",
    "categories = [\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey']\n",
    "\n",
    "trainingset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=36)\n",
    "# Terminology note: training set - data used to by machine learning model to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traingingset is a sklearn \"bunch\" object\n",
    "# The values were are trying to predict is stored in the \"target_names\" attribute\n",
    "trainingset.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A sample article:\n",
      "##------------------------------------\n",
      "Organization: Penn State University\n",
      "From: Robbie Po <RAP115@psuvm.psu.edu>\n",
      "Subject: Re: Devils and Islanders tiebreaker????\n",
      "Lines: 14\n",
      "\n",
      "In article <C5LDI2.77u@odin.corp.sgi.com>, enolan@sharkbite.esd.sgi.com (Ed\n",
      "Nolan) says:\n",
      ">If the Islanders beat the Devils tonight, they would finish with\n",
      ">identical records.  Who's the lucky team that gets to face the Penguins\n",
      ">in the opening round?   Also, can somebody list the rules for breaking\n",
      ">ties.\n",
      "      As I recall, the Penguins and Devils tied for third place last year\n",
      "with identical records, as well.  Poor Devils -- they always get screwed.\n",
      "Yet, they should put a scare into Pittsburgh.  They always do!  Pens in 7.\n",
      "-------------------------------------------------------------------------\n",
      "** Robbie Po **          PGH PENGUINS!!!    \"It won't be easy, but it\n",
      "Contact for the '93-'94  '91 STANLEY CUP    will have greater rewards.\n",
      "Penn State Lady Lions    '92 CHAMPIONS      Mountains and Valleys are\n",
      "rap115@psuvm.psu.edu     11 STRAIGHT WINS!  better than nothing at all!\"\n",
      "\n",
      "##------------------------------------\n",
      "\n",
      "\n",
      "The sample messages's category index: 3, which is really category: 'rec.sport.hockey'\n",
      "\n",
      "Total messages:2389\n",
      "  category: rec.autos            number of messages: 594\n",
      "  category: rec.motorcycles      number of messages: 598\n",
      "  category: rec.sport.baseball   number of messages: 597\n",
      "  category: rec.sport.hockey     number of messages: 600\n"
     ]
    }
   ],
   "source": [
    "# The training data (messages) are under the \"data\" attribute\n",
    "print(\"\\nA sample article:\\n##------------------------------------\\n{}\\n##------------------------------------\\n\\n\".format(trainingset.data[0]))\n",
    "\n",
    "# Can you predict the category based on the message? will the machine be able to??\n",
    "\n",
    "# The index of the correct categories are available under the \"target\" attribute\n",
    "print(\"The sample messages's category index: {}, which is really category: '{}'\\n\".format(\n",
    "    trainingset.target[0], trainingset.target_names[trainingset.target[0]]))\n",
    "# Here is the size of the data were working with\n",
    "print(\"Total messages:{}\".format(len(trainingset.data)))\n",
    "counts = np.unique(trainingset.target, return_counts=True)\n",
    "[print(\"  category: {:<20} number of messages: {}\".format(trainingset.target_names[cat],n)) for cat,n in zip(counts[0],counts[1])]\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to convert a message into a format that can be used by the machine learning algorithm\n",
    "# ...called feature extraction\n",
    "\n",
    "# The \"bag of words\" approach\n",
    "\n",
    "# Convert the messages into a matrix:\n",
    "# Each column is mapped to a word\n",
    "# Each row is mapped to a message\n",
    "# The matrix value represents the number of times that word appeared in the message\n",
    "\n",
    "# E.g. \n",
    "# The sklearn CountVectorizer does this for us:\n",
    "example_messages = [\"Hello PyYYC, I say again Hello\",\"Welcome to pyyyc\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Some method names will reappear over and over in sklearn. These are part of the sklearn specification\n",
    "# \"fit\", \"transform\" and \"fit_transform\" are examples\n",
    "# fit: \"learn\" or \"fit\" the data by recording every instance of word and assigning it a column.\n",
    "#   Learned data is stored in the object and nothing is returned.\n",
    "# transform: using the \"fitted\" model saved in the object, convert the input data into a new format\n",
    "# fit_transform: does both in one step\n",
    "word_counts = vectorizer.fit_transform(example_messages)\n",
    "word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 2x6 sparse matrix\n",
    "# There's more than 6 words, what's going on??\n",
    "\n",
    "# In CountVectorizer, the default is for words to >1 letters, and all strings are converted to lowercase\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "# What's a sparse matrix\n",
    "# Many messages will have 0 instances of word. When each word has separate column this means that many of the \n",
    "# matrix entries will be zero AND it will be large.\n",
    "# A sparse matrix only keeps track of non-zero entries to save space\n",
    "# On the surface, it works just like a normal matrix\n",
    "print(word_counts[0,1])\n",
    "print(word_counts[0,5])\n",
    "print(word_counts[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'hello', 'pyyyc', 'say', 'to', 'welcome']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# What words are the columns in matrix mapped to?\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# You can lookup the index of a work like so:\n",
    "print(vectorizer.vocabulary_.get('pyyyc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389, 30446)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So lets perform feature extraction on our messages\n",
    "word_counts = vectorizer.fit_transform(trainingset.data)\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d1u',\n",
       " 'd6n',\n",
       " 'd6s',\n",
       " 'd90',\n",
       " 'd96',\n",
       " 'd_jaracz',\n",
       " 'da',\n",
       " 'da_tinker1',\n",
       " 'daaaaaaaaaaaaaaaaaaaaaay',\n",
       " 'dab']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389, 17830)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hmmm, looks like lots of quality informative words there\n",
    "# We could try to train our machine learning models with this data\n",
    "# Or we maybe we can improve our vectorized training data a bit by stripping out low quality data\n",
    "\n",
    "# Feature selection\n",
    "\n",
    "# What if we just focus on words that appear in more than one document\n",
    "# We can use an option \"min_df\" in CountVectorizer for this\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "word_counts = vectorizer.fit_transform(trainingset.data)\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tny',\n",
       " 'to',\n",
       " 'tobaccos',\n",
       " 'tobias',\n",
       " 'toby',\n",
       " 'tocchet',\n",
       " 'tochett',\n",
       " 'tod',\n",
       " 'today',\n",
       " 'todays']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[16260:16270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389, 17830)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about the word \"to\". Is that helpful?\n",
    "# Very frequent words like \"like\", \"to\", \"and\" should probably be removed\n",
    "\n",
    "# And another issue, longer messages will have higher word counts. How do we normalize for message size?\n",
    "\n",
    "# We can use a feature selection technique called “Term Frequency times Inverse Document Frequency”.\n",
    "# Term frequency adjusts for the length of message dividing word occurances by total number of words\n",
    "# Inverse document frequency downweights words that appear in many documents (the \"to\"s)\n",
    "\n",
    "# Lets run our count data through a TF-IDF transformation:\n",
    "tfidf_transformer = TfidfTransformer().fit(word_counts)\n",
    "adjusted_word_counts = tfidf_transformer.transform(word_counts)\n",
    "adjusted_word_counts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.toarray()[0,8740:8850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18462397, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02909454, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05655349, 0.        ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_word_counts.toarray()[0,8740:8850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I love to score goals' => rec.sport.hockey\n",
      "'I love to hit home runs' => rec.sport.baseball\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/envs/pyyyc_sklearn/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Finally, data is ready.\n",
    "# Lets teach a machine to predict a category from a newsgroup message:\n",
    "\n",
    "# Using defaults\n",
    "classifier = SGDClassifier()\n",
    "\n",
    "# Give it our features, and the correct answers\n",
    "classifier.fit(adjusted_word_counts, trainingset.target)\n",
    "\n",
    "# And now lets use it to predict categories on messages the classifier has never seen\n",
    "# Don't forget to perform the same feature selection as before\n",
    "test_messages = [\"I love to score goals\", \"I love to hit home runs\"]\n",
    "test_word_counts = vectorizer.transform(test_messages)\n",
    "test_adj_word_counts = tfidf_transformer.transform(test_word_counts)\n",
    "predictions = classifier.predict(test_adj_word_counts)\n",
    "\n",
    "# Is the machine right?\n",
    "for msg, category in zip(test_messages, predictions):\n",
    "    print('%r => %s' % (msg, trainingset.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems good\n",
    "# But just how good?\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
